{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red255\green219\blue205;\red26\green26\blue26;\red10\green0\blue109;
}
\margl1440\margr1440\vieww16660\viewh17620\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 INSTALLATION:\
gcl pocket && base\
./autogen.sh\
./config \'97prefix\
make\
make install\
\
>> add PATH\
>> PKG_CONFIG_PATH=/nfs/2017/n/nsabbah/Documents/KIFT/base\
\
record:\
sox -d filename.wav\
play:\
play filename.wav\
\
\
export PATH=/nfs/2017/n/nsabbah/.brew/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/munki:/nfs/2017/n/nsabbah/Documents/KIFT/pocket/bin:/nfs/2017/n/nsabbah/Documents/KIFT/pocket/bin:/nfs/2017/n/nsabbah/Documents/KIFT/base/bin:/nfs/2017/n/nsabbah/Documents/KIFT/train/bin\
\
\

\b INTRODUCTION \'97\'97\'97\'97\'97\'97\

\b0 \
\pard\pardeftab720\sl340\partightenfactor0
\cf0 Speech recognition system produces a text output corresponding to the given speech input. A speaker-dependent (SD) recognition system results in a higher recognition performance when compared to a speaker-independent (SI) system. Speaker adaptation techniques like maximum aposteriori (MAP) and maximum likelihood linear regression (MLLR) are applied to an SI system, in order to get a recognition performance similar to that of an SD system, with minimal amount of data\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 >> {\field{\*\fldinst{HYPERLINK "http://ieeexplore.ieee.org/document/7054938/?reload=true"}}{\fldrslt http://ieeexplore.ieee.org/document/7054938/?reload=true}}\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\qc\partightenfactor0

\fs28 \cf0 >>>>> https://cmusphinx.github.io/wiki/tutorialadapt/ <<<<<<
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
Copy the default acoustic model\
\cb2 	cp -a /usr/local/share/pocketsphinx/model/en-us/en-us .\
\cb1 \
Lets also copy the dictionary and the language model for the testing\
\cb2 	cp -a /usr/local/share/pocketsphinx/model/en-us/cmudict-en-us.dict .\
	cp -a /usr/local/share/pocketsphinx/model/en-us/en-us.lm.bin .\
\cb1 \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
Convert .wav file to a set of acoustic model // acoustic feature files\
\pard\pardeftab720\sl320\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
An 
\b acoustic model
\b0  is used in {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Automatic_Speech_Recognition"}}{\fldrslt \cf4 Automatic Speech Recognition}} to represent the relationship between an {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Audio_signal"}}{\fldrslt \cf4 audio signal}} and the {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Phonemes"}}{\fldrslt \cf4 phonemes}} or other linguistic units that make up speech. The model is learned from a set of audio recordings and their corresponding transcripts. It is created by taking audio recordings of speech, and their text transcriptions, and using software to create statistical representations of the sounds that make up each word.\
{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Acoustic_model"}}{\fldrslt https://en.wikipedia.org/wiki/Acoustic_model}}\cf0 \kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\cb2 sphinx_fe -argfile en-us/feat.params \\\
        -samprate 16000 -c songs.fileids \\\
       -di . -do . -ei wav -eo mfc -mswav yes\
\cb1 \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
Collect statistics from the adaptation data\
\
\cb2 ./bw -hmmdir en-us -moddeffn en-us/mdef.txt -ts2cbfn .cont. -feat 1s_c_d_dd -cmn current -agc none -lda en-us/feature_transform -dictfn cmudict-en-us.dict -ctlfn songs.fileids -lsnfn songs.transcription -accumdir .\
\cb1 \
hmm stands for Hidden Markov model\
\
OPTION 1 MLLR \'97\'97\'97\'97\'97\'97\
\
MLLR (Maximum Likelihood Linear Regression) is a cheap adaptation method that is suitable when amount of data is limited. It\'92s a good idea to use MLLR for online adaptation. MLLR works best for continuous model.\
\
\cb2 ./../train/libexec/sphinxtrain/mllr_solve -meanfn en-us/means -varfn en-us/variances -outmllrfn mllr_matrix -accumdir .\
\cb1 \
How to run it?\
\
This command will create an adaptation data file called mllr_matrix. Now, if you wish to decode with the adapted model, simply add -mllr mllr_matrix (or whatever the path to the mllr_matrix file you created is) to your pocketsphinx command line.\
\
OPTION 2 MAP\'97\'97\'97\'97\'97\'97\'97\
\
MAP (Maximum aposteriori) model\
\
\cb2 cp -a en-us en-us-adapt\
./map_adapt  -moddeffn en-us/mdef.txt  -ts2cbfn .ptm.  -meanfn en-us/means  -varfn en-us/variances  -mixwfn en-us/mixture_weights  -tmatfn en-us/transition_matrices  -accumdir .  -mapmeanfn en-us-adapt/means  -mapvarfn en-us-adapt/variances  -mapmixwfn en-us-adapt/mixture_weights  -maptmatfn en-us-adapt/transition_matrices\
\cb1 \
If you want to save space for the model you can use sendump file supported by pocketsphinx. For sphinx4 you don\'92t need that. To recreate the sendump file from the updated mixture_weights file:\
\
\cb2 ./mk_s2sendump  -pocketsphinx yes  -moddeffn en-us-adapt/mdef.txt  -mixwfn en-us-adapt/mixture_weights  -sendumpfn en-us-adapt/sendump\
\cb1 \
TEST \'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
\cb2 pocketsphinx_continuous -hmm en-us-adapt -lm en-us.lm.bin -infile fileinput.wav > input_transcription.text\cb1 \
\
OR\
\
pocketsphinx_continuous -hmm en-us-adapt -lm en-us.lm.bin -inmic yes\
\
\
\
TO READ \'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
{\field{\*\fldinst{HYPERLINK "https://cmusphinx.github.io/wiki/tutoriallm/"}}{\fldrslt https://cmusphinx.github.io/wiki/tutoriallm/}}\
}